{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chronological Drift in the Ember Dataset\n",
    "\n",
    "This example demonstrates the drift tracking features of Goko. \n",
    "\n",
    "We start by importing the correct things, and sorting the ember dataset (training + test) by appearence date. We build the covertree on everything before July 2018, and then feed the data in chrological order to the covertree and track the KL divergence.\n",
    "\n",
    "The major flaw with this experiment is the lack of an actual test set. The test set only consists of a couple of months, and does not overlap with the training so is not suited for this analysis. We have done this same experiment with a proper test/train split and it produces identical results.\n",
    "\n",
    "We start by importing the data and sorting it into it's parts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ember\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pygoko import CoverTree\n",
    "\n",
    "\n",
    "X, _  = ember.read_vectorized_features(datadir,\"train\")\n",
    "metadata = pd.read_csv(os.path.join(datadir, \"train_metadata.csv\"), index_col=0)\n",
    "\n",
    "split_date = \"2018-07\"\n",
    "all_dates = list(set(metadata['appeared']))\n",
    "all_dates.sort()\n",
    "training_dates = [d for d in all_dates if d < split_date]\n",
    "\n",
    "X_by_month = {k:X[metadata['appeared'] == k] for k in all_dates}\n",
    "\n",
    "training_data = np.concatenate([X_by_month[k] for k in training_dates])\n",
    "training_data = np.ascontiguousarray(training_data)\n",
    "\n",
    "all_data = np.concatenate([X_by_month[k] for k in all_dates])\n",
    "all_data = np.ascontiguousarray(all_data)"
   ]
  },
  {
   "source": [
    "## Building the tree\n",
    "\n",
    "We have 1m samples so we can pick a decent size for the leaf cutoff of 50. The features are nearly all integers, except for the byte histogram and entropy, so a minimum resolution index of 0 ignores the minor noise that these introduce."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = CoverTree()\n",
    "tree.set_leaf_cutoff(50)\n",
    "tree.set_scale_base(1.5)\n",
    "tree.set_min_res_index(0)\n",
    "tree.fit(training_data)"
   ]
  },
  {
   "source": [
    "## Gather a baseline\n",
    "\n",
    "We gather a baseline object. When you feed the entire dataset the covertree was created from to itself, \n",
    "you will get a non-zero KL-Div on any node that is non-trivial. This process will weight the node's posterior Dirichlet distribution,\n",
    "multiplying the internal weights by (prior_weight + observation_weight). This posterior distribution has a lower variance than the prior and   \n",
    "the expected KL-divergence between the unknown distributions we're modeling is thus non-zero.\n",
    "\n",
    "This slowly builds up, but we expect a non-zero KL-div over the nodes as we feed in-distribution data in. This object estimates that, and\n",
    "allows us to normalize this natural variance away. \n",
    "\n",
    "We use a window size of 5000, so the posterior is built off the prior and the last 5000 elements of the sequence. The sequence length is set to be \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5000   \n",
    "sequence_len = 800000\n",
    "# Actually computes the KL div this often. All other values are linearly interpolated between these sample points.\n",
    "# It's too slow to calculate each value and this is accurate enough.\n",
    "sample_rate = 500\n",
    "# Gets the mean and variance over this number of simulated sequence. \n",
    "sequence_count = 50\n",
    "\n",
    "baseline = tree.kl_div_dirichlet_baseline(\n",
    "    1.0,\n",
    "    1.0,\n",
    "    window_size,  \n",
    "    sequence_count,\n",
    "    sample_rate)\n",
    "def normalize(baseline,stats):\n",
    "    \"\"\"\n",
    "    Grabs the mean and variance from the baseline and normalizes the stats object passed in by subtracting \n",
    "    the norm and dividing by the standard deviation.\n",
    "    \"\"\"\n",
    "    basesline_stats = baseline.stats(stats[\"sequence_len\"])\n",
    "    normalized = {}\n",
    "    for k in basesline_stats.keys():\n",
    "        n = (stats[k]-basesline_stats[k][\"mean\"])\n",
    "        if basesline_stats[k][\"var\"] > 0:\n",
    "            n = n/np.sqrt(basesline_stats[k][\"var\"])\n",
    "        normalized[k] = n\n",
    "    return normalized\n"
   ]
  },
  {
   "source": [
    "This is the actual object that computes the KL Divergence statistics between the samples we feed in and the new samples. \n",
    "\n",
    "Internally, it is an evidence hashmap containing categorical distributions, and a queue of paths. \n",
    "The sample's path is computed, we then push it onto the queue and update the evidence by incrementing the correct buckets \n",
    "in the evidence hashmap. If the queue is full, we pop off the oldest path and decrement the correct paths in the queue."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tracker = tree.kl_div_dirichlet(\n",
    "    prior_weight,\n",
    "    observation_weight,\n",
    "    window_size)\n",
    "\n",
    "total_kl_div = []\n",
    "for i,datum in enumerate(all_data):\n",
    "    run_tracker.push(datum)\n",
    "    if i % 500 == 0:\n",
    "        kl_div = normalize(baseline.stats(),run_tracker.stats())['moment1_nz']\n",
    "        total_kl_div.append(kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(range(0,len(all_data),500)),total_kl_div)\n",
    "ax.set_ylabel('KL Divergence')\n",
    "ax.set_xlabel('Sample Timestamp')\n",
    "tick_len = 0\n",
    "cutoff_len = 0\n",
    "tick_locations = []\n",
    "dates = [d for d in X_month.keys()]\n",
    "for date in dates:\n",
    "    if date == \"2018-07\":\n",
    "        cutoff_len = tick_len\n",
    "    tick_len += len(X_month[date])\n",
    "    tick_locations.append(tick_len)\n",
    "ax.set_xticks(tick_locations)\n",
    "ax.set_xticklabels(dates)\n",
    "ax.axvline(x=cutoff_len, linewidth=4, color='r')\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"drift.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ]
}